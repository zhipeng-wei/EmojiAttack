import pandas as pd
import os
import torch
import json
from tqdm import tqdm
from easyjailbreak.models.huggingface_model import from_pretrained
from easyjailbreak.models.openai_model import OpenaiModel
from easyjailbreak.attacker.AutoDAN_Liu_2023 import autodan_PrefixManager, load_conversation_template, generate
from easyjailbreak.mutation.rule.BinaryTree import BINARY_TREE
from easyjailbreak.mutation.rule.Reverse import REVERSE
from easyjailbreak.mutation.rule.OddEven import ODD_EVEN
from easyjailbreak.mutation.rule.Length import LENGTH

if __name__ == '__main__':
    # ICA achieves 0% ASR, and Cipher requires the additional translation on responses.
    # Hence, we omit these two attacks, following http://easyjailbreak.org/leader-board.
    base_path = './EasyJailbreaking-Results'
    attack_methods = ['Autodan', 'CodeChameleon', 'Deepinception', 'gcg', 'GPTFuzz', 'Jailbroken', 'Multilingual', 'PAIR', 'ReNellm']
    target_model_file_name = 'chatgpt.jsonl'

    openai_key = os.getenv('OPENAI_API_KEY')
    target_model = OpenaiModel('gpt-3.5-turbo', openai_key)
    adv_bench = pd.read_csv('./advbench/harmful_behaviors.csv')

    saved_base_path = os.getenv('OUTPUT_BASE_PATH', './')
    saved_path = os.path.join(saved_base_path, 'BlackAttack_gpt-3.5-turbo')
    if not os.path.exists(saved_path):
        os.makedirs(saved_path)

    for attack_method in attack_methods:
        used_path = os.path.join(base_path, attack_method, target_model_file_name)
        infos = pd.read_json(path_or_buf=used_path, lines=True)
        infos['is_success'] = infos['eval_results'].apply(lambda x:x[0])
        infos['is_success'] = infos['is_success'].astype(bool)
        success_infos = infos[infos['is_success']]

        # The results on Multilingual have some items without translated queries.
        if attack_method == 'Multilingual':
            success_infos = success_infos[~success_infos['translated_query'].isna()]

        print (f'Runing {attack_method}, it has {success_infos.shape[0]} examples.')

        saved_info = []
        for idx, row in tqdm(success_infos.iterrows(), desc='Generating Responses', total=success_infos.shape[0]):
            item = row['jailbreak_prompt']
            query = row['query']
            # target = adv_bench[adv_bench['goal']==query]['target'].values.tolist()[0]
            off_response = row['target_responses']
            is_success = row['eval_results']

            if attack_method == 'Multilingual':
                # Multilingual requires the translated queries.
                translated_query = row['translated_query']
                key, value = '{translated_query}', translated_query
            else:
                key, value = '{query}', query
            
            if key in item:
                if attack_method == 'CodeChameleon':
                    # CodeChameleon has four differnet decryption functions.
                    if isinstance(value, dict):
                        used_prompt = item.format(decryption_function=BINARY_TREE, query=value)
                    elif isinstance(value, list):
                        used_prompt = item.format(decryption_function=LENGTH, query=value)
                    elif isinstance(value, str):
                        if value[0].isupper():
                            used_prompt = item.format(decryption_function=ODD_EVEN, query=value)
                        else:
                            used_prompt = item.format(decryption_function=REVERSE, query=value)
                else:
                    try:
                        used_prompt = item.replace(key, value)
                    except:
                        # Check what happened.
                        raise ValueError(f"{key}={value}")
            else:
                if attack_method not in ['Deepinception', 'PAIR']:
                    used_prompt = item + value
                else:
                    # Deepinception and PAIR don't need query information.
                    used_prompt = item
            response = target_model.generate(used_prompt, temperature=0)
            saved_info.append({
                'attack': attack_method,
                'used_prompt': used_prompt,
                'response': response
            })
        with open(os.path.join(saved_path, f'{attack_method}-NormalResponse.json'), 'w') as opt:
            json.dump(saved_info, opt)

