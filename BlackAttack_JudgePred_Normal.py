import argparse
import os
import glob
import json
import numpy as np
import pickle as pkl
from judge.ShieldLM import ShieldLLM_reference
from judge.WildGuard import WildGurad_reference
from judge.LlamaGuard import LlamaGuard_reference
from judge.LlamaGuard2 import LlamaGuard2_reference
from judge.gpt35 import gpt35_reference
from judge.match import match_reference
from judge.gpt4 import gpt4_reference
from judge.gemini import gemini_reference
from judge.Claude import claude_reference
from judge.deepseek import deepseek_reference
from judge.o3mini import o3mini_reference

if __name__ == '__main__':
    output_base_name = os.getenv('OUTPUT_BASE_PATH', './')
    normal_output_folder = 'BlackAttack_gpt-3.5-turbo'
    used_path = os.path.join(output_base_name, normal_output_folder)

    process_files = glob.glob(os.path.join(used_path, '*-NormalResponse.json'))
    
    all_used_ids = []
    all_attack_name = []
    all_attack_prompts = []
    all_responses = []
    for one_file in process_files:
        attack_method = one_file.split('/')[-1].split('-')[0]
        with open(one_file, 'r') as ipt:
            infos = json.load(ipt)
        
        match_pres = match_reference([infos[idx]['response'] for idx in range(len(infos))])
        match_pres = np.array(match_pres)
        used_idx = np.where(match_pres == 1)[0]

        print (f'Using attack {attack_method}, After filting by matching, the number of examples is changed to {len(used_idx)} from {len(infos)}.')
        if len(used_idx) == 0:
            continue
        
        attack_name_list = [infos[idx]['attack'] for idx in used_idx] 
        attack_prompts_list = [infos[idx]['used_prompt'] for idx in used_idx] 
        responses_list = [infos[idx]['response'] for idx in used_idx]

        all_used_ids += list(used_idx)
        all_attack_name += attack_name_list
        all_attack_prompts += attack_prompts_list
        all_responses += responses_list
    
    print (f'The number of all examples is {len(all_used_ids)}.')
    save_dict = []
    for t_i, idx in enumerate(all_used_ids):
        save_dict.append(
            {
                'used_idx': idx,
                'attack': all_attack_name[t_i],
                'used_prompt': all_attack_prompts[t_i],
                'response': all_responses[t_i]
            }
        )
    # open-source judge llms
    for func in [ShieldLLM_reference, WildGurad_reference, LlamaGuard_reference, LlamaGuard2_reference]:
        pres, conts = func(all_responses, batch_size=1)
        for t_i,idx in enumerate(all_used_ids):
            save_dict[t_i][f'{func.__name__}-Pre'] = pres[t_i]
            save_dict[t_i][f'{func.__name__}-Detail'] = conts[t_i]

    output_name = os.path.join(used_path, 'Open-source-NormalJudge.pkl')
    with open(output_name, 'wb') as opt:
        pkl.dump(save_dict, opt)

    # commercial judge llms
    for func in [gpt35_reference, gpt4_reference, gemini_reference, claude_reference, deepseek_reference, o3mini_reference]:
        pres, conts = func(all_responses, batch_size=1, output_path=os.path.join(used_path, f'Table1_Normal_CostAPI_-{func.__name__}.jsonl'))
