# Offensive Words are from https://github.com/LDNOOBW/List-of-Dirty-Naughty-Obscene-and-Otherwise-Bad-Words/blob/master/en.
# In this file, we wanna obtain the prediction from judge llms before and after spliting the offensive words.
import pickle as pkl
import torch
from judge.ShieldLM import ShieldLLM_reference
from judge.WildGuard import WildGurad_reference
from judge.LlamaGuard import LlamaGuard_reference, response_Llama_guard_prompt
from judge.LlamaGuard2 import LlamaGuard2_reference
from judge.gpt35 import gpt35_reference
from judge.gpt4 import gpt4_reference
from sentence_transformers import SentenceTransformer
from transformers import AutoTokenizer, AutoModelForCausalLM
from tqdm import tqdm
import numpy as np
import os

def read_phrases(file_path):
    # read phrases from file
    with open(file_path, 'r', encoding='utf-8') as file:
        phrases = [line.strip() for line in file]
    return phrases

def split_words(phrases):
    # split words in phrases with middle point
    split_phrases = []
    for phrase in phrases:
        words = phrase.split()
        split_words = [' '.join([word[:len(word)//2], word[len(word)//2:]]) for word in words]
        split_phrases.append(' '.join(split_words))
    return split_phrases

def split_words_cosine(emb_model, phrase):
    words = phrase.split()
    split_words = []
    for word in words:
        if len(word) == 1:
            split_words.append(word)
            continue
        ori_embedding = emb_model.encode([word])
        cs_outputs = []
        for i in range(len(word)-1):
            split_embedding = emb_model.encode([word[:i+1], word[i+1:]])
            cs_output = torch.nn.functional.cosine_similarity(torch.tensor(split_embedding), torch.tensor(ori_embedding))
            cs_outputs.append(cs_output.mean().item())
        order_token_inds = torch.argsort(torch.tensor(cs_outputs), descending=False)
        used_pos = order_token_inds[0]+1
        split_word = ' '.join([word[:used_pos], word[used_pos:]])
        split_words.append(split_word)
    return ' '.join(split_words)

def paraphrase_cosine(emb_model, phrases):
    split_phrases = []
    for phrase in phrases:
        split_phrase = split_words_cosine(emb_model, phrase)
        split_phrases.append(split_phrase)
    return split_phrases

if __name__ == '__main__':
    file_path = './datasets/en.txt'
    phrases = read_phrases(file_path)
    # since the final one is one emoji, we remove it, and dicuss emojis in another file.
    phrases = phrases[:-1]
    splited_phrases = split_words(phrases)

    
    tokenizer = AutoTokenizer.from_pretrained(os.getenv('LLAMA_GUARD_PATH'))
    model = AutoModelForCausalLM.from_pretrained(os.getenv('LLAMA_GUARD_PATH'), torch_dtype=torch.bfloat16, device_map='auto').eval()
    all_probs = []
    for i in tqdm(range(len(splited_phrases)), desc="Running LlamaGuard"):
        input_text = response_Llama_guard_prompt.format(response=splited_phrases[i])
        input_ids = tokenizer.encode(input_text, return_tensors="pt").to(model.device)
        with torch.no_grad():
            outputs = model(input_ids)

        logits = outputs.logits
        unsafe_target_token = "unsafe"
        unsafe_target_token_id = tokenizer.encode(unsafe_target_token, add_special_tokens=False)[0]
        
        safe_target_token = "safe"
        safe_target_token_id = tokenizer.encode(safe_target_token, add_special_tokens=False)[0]
        
        last_token_logits = logits[0, -1, :]
        unsafe_logit = last_token_logits[unsafe_target_token_id].item()
        safe_logit = last_token_logits[safe_target_token_id].item()
        probs = torch.softmax(torch.Tensor([unsafe_logit, safe_logit]).to(model.device), dim=-1)
        all_probs.append(probs[0].item())
    np.save('offensive_unsafe_probs.npy', np.array(all_probs))
