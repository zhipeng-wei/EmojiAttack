# Following Sec 5.1 of the paper "PRP: Propagating Universal Perturbations to Attack Large Language Model Guard-Rails"ï¼Œ
# We obtain benign prompts' responses by unaligned Mistral-7B-Instruct-v0.1, so as to perform in-context learning.
import pandas as pd
import re
import math
from transformers import AutoModelForCausalLM, AutoTokenizer
from tqdm import tqdm
import os
import json

def read_text(filename):
    string = []
    with open(filename, "r") as f:
        full_text = f.read()
        for l in re.split(r"\n", full_text):
            string.append(l)
    return string

def write_text(filename, lines):
    with open(filename, "w") as f:
        for line in lines:
            f.write(line + "\n")

# need to set the cache directory and token for Hugging Face models
MyToken = os.getenv('MyToken')
CACHEDIR = os.getenv('CACHEDIR')
SAFE_DATAFILE = './in-context-data/safe_prompts.txt'
HARMFUL_DATAFILE = './in-context-data/harmful_prompts.txt'
SAFE_OUTPUTFILE = './in-context-data/safe_responses.json'
HARMFUL_OUTPUTFILE = './in-context-data/harmful_responses.json'


if __name__ == '__main__':
    os.environ["CUDA_VISIBLE_DEVICES"] = '6,7'
    safe_prompts = read_text(filename=SAFE_DATAFILE)
    harmful_prompts = read_text(filename=HARMFUL_DATAFILE)

    model = AutoModelForCausalLM.from_pretrained("mistralai/Mistral-7B-Instruct-v0.1", cache_dir=CACHEDIR, token=MyToken, device_map='auto')
    tokenizer = AutoTokenizer.from_pretrained("mistralai/Mistral-7B-Instruct-v0.1", cache_dir=CACHEDIR, token=MyToken)

    model.eval()
    
    batch_size = 1
    re_json = {}
    for i in tqdm(range(math.ceil(len(safe_prompts)/batch_size))):
        batch_prompts = safe_prompts[i*batch_size:min((i+1)*batch_size, len(safe_prompts))]
        input_ids = tokenizer(batch_prompts, return_tensors='pt').to(model.device)
        prompt_len = input_ids["input_ids"].shape[-1]
        generated_ids = model.generate(**input_ids, max_new_tokens=100, do_sample=True)
        for b_id, generated_id in enumerate(generated_ids):
            result = tokenizer.decode(generated_id.tolist()[prompt_len:])
            re_json[batch_prompts[b_id]] = result

    with open(SAFE_OUTPUTFILE, 'w') as opt:
        json.dump(re_json, opt)

    re_json = {}
    for i in tqdm(range(math.ceil(len(harmful_prompts)/batch_size))):
        batch_prompts = harmful_prompts[i*batch_size:min((i+1)*batch_size, len(harmful_prompts))]
        input_ids = tokenizer(batch_prompts, return_tensors='pt').to(model.device)
        prompt_len = input_ids["input_ids"].shape[-1]
        generated_ids = model.generate(**input_ids, max_new_tokens=100, do_sample=True)
        for b_id, generated_id in enumerate(generated_ids):
            result = tokenizer.decode(generated_id.tolist()[prompt_len:])
            re_json[batch_prompts[b_id]] = result

    with open(HARMFUL_OUTPUTFILE, 'w') as opt:
        json.dump(re_json, opt)


